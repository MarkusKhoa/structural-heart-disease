{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Structural Heart Disease Multimodal Model - Training on Google Colab\n",
    "\n",
    "This notebook trains a multimodal deep learning model for structural heart disease prediction using:\n",
    "- **A1**: ECG Transformer Encoder (HuBERT-ECG pretrained)\n",
    "- **A2**: Tabular Encoder (FTTransformer)\n",
    "- **A3**: Gated Multimodal Fusion + Multi-label Prediction\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Setup & Installation](#setup)\n",
    "2. [Data Upload/Mounting](#data)\n",
    "3. [Model Configuration](#config)\n",
    "4. [Training](#training)\n",
    "5. [Evaluation & Visualization](#evaluation)\n",
    "6. [Download Results](#download)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## 1. Setup & Installation\n",
    "\n",
    "First, let's check if we're running on Colab and set up GPU acceleration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNING: No GPU detected. Training will be slow on CPU.\")\n",
    "    print(\"Go to Runtime > Change runtime type > Hardware accelerator > GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "print(\"Installing dependencies...\")\n",
    "!pip install -q transformers>=4.30.0\n",
    "!pip install -q tab-transformer-pytorch>=0.2.0\n",
    "!pip install -q tensorboard\n",
    "\n",
    "print(\"‚úì Installation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "### Clone Repository (Option 1: From GitHub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "git_clone"
   },
   "outputs": [],
   "source": [
    "# If your code is on GitHub, clone it here\n",
    "# Uncomment and modify the following lines:\n",
    "\n",
    "# !git clone https://github.com/YOUR_USERNAME/structural_heart_disease.git\n",
    "# %cd structural_heart_disease\n",
    "\n",
    "# For now, we'll create the necessary files directly\n",
    "print(\"Skipping git clone - will upload files manually or mount from Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_code"
   },
   "source": [
    "### Upload Code Files (Option 2: Manual Upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_structure"
   },
   "outputs": [],
   "source": [
    "# Create project structure\n",
    "import os\n",
    "\n",
    "os.makedirs('src', exist_ok=True)\n",
    "os.makedirs('echonext_dataset', exist_ok=True)\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "\n",
    "print(\"‚úì Project structure created\")\n",
    "print(\"\\nüìÅ Please upload the following files:\")\n",
    "print(\"  - src/models.py\")\n",
    "print(\"  - src/dataset.py\")\n",
    "print(\"  - src/utils.py\")\n",
    "print(\"  - src/__init__.py\")\n",
    "print(\"\\nUse the file upload button on the left sidebar or run the cell below.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upload_files"
   },
   "outputs": [],
   "source": [
    "# Upload source files\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Upload src/models.py:\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} src/models.py\n",
    "\n",
    "print(\"\\nUpload src/dataset.py:\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} src/dataset.py\n",
    "\n",
    "print(\"\\nUpload src/utils.py:\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    !mv {filename} src/utils.py\n",
    "\n",
    "# Create __init__.py\n",
    "!touch src/__init__.py\n",
    "\n",
    "print(\"\\n‚úì Source files uploaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "data"
   },
   "source": [
    "## 2. Data Upload/Mounting\n",
    "\n",
    "Choose one of the following options to access your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "### Option A: Mount Google Drive (Recommended for large datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive_mount"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Update this path to where your data is stored in Google Drive\n",
    "DRIVE_DATA_PATH = '/content/drive/MyDrive/echonext_dataset'\n",
    "\n",
    "# Create symbolic link\n",
    "!ln -s {DRIVE_DATA_PATH} echonext_dataset\n",
    "\n",
    "print(f\"‚úì Data mounted from: {DRIVE_DATA_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "upload_data"
   },
   "source": [
    "### Option B: Upload Data Files Manually (For smaller datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "manual_upload"
   },
   "outputs": [],
   "source": [
    "# Upload data files one by one\n",
    "# WARNING: This can be slow for large .npy files!\n",
    "\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "print(\"Upload EchoNext_metadata_100k.csv:\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, 'echonext_dataset/EchoNext_metadata_100k.csv')\n",
    "\n",
    "print(\"\\nUpload training waveforms (.npy):\")\n",
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, 'echonext_dataset/EchoNext_train_waveforms.npy')\n",
    "\n",
    "# Continue for other files...\n",
    "print(\"\\n‚ö†Ô∏è Note: Upload remaining .npy files using the same pattern\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_data"
   },
   "source": [
    "### Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_data"
   },
   "outputs": [],
   "source": [
    "# Check if all required files exist\n",
    "import os\n",
    "\n",
    "required_files = [\n",
    "    'echonext_dataset/EchoNext_metadata_100k.csv',\n",
    "    'echonext_dataset/EchoNext_train_waveforms.npy',\n",
    "    'echonext_dataset/EchoNext_train_tabular_features.npy',\n",
    "    'echonext_dataset/EchoNext_val_waveforms.npy',\n",
    "    'echonext_dataset/EchoNext_val_tabular_features.npy',\n",
    "    'echonext_dataset/EchoNext_test_waveforms.npy',\n",
    "    'echonext_dataset/EchoNext_test_tabular_features.npy',\n",
    "]\n",
    "\n",
    "print(\"Checking data files:\")\n",
    "all_present = True\n",
    "for filepath in required_files:\n",
    "    exists = os.path.exists(filepath)\n",
    "    status = \"‚úì\" if exists else \"‚úó\"\n",
    "    size = f\"{os.path.getsize(filepath) / 1e6:.1f} MB\" if exists else \"Missing\"\n",
    "    print(f\"{status} {filepath}: {size}\")\n",
    "    if not exists:\n",
    "        all_present = False\n",
    "\n",
    "if all_present:\n",
    "    print(\"\\n‚úì All data files present!\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some files are missing. Please upload them before continuing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config"
   },
   "source": [
    "## 3. Model Configuration\n",
    "\n",
    "Configure training hyperparameters and model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "config_params"
   },
   "outputs": [],
   "source": [
    "# Training Configuration\n",
    "config = {\n",
    "    # Data\n",
    "    'data_dir': './echonext_dataset',\n",
    "    'batch_size': 16,  # Reduced for Colab GPU memory\n",
    "    'num_workers': 2,  # Colab has limited CPU cores\n",
    "    \n",
    "    # Model - ECG Encoder\n",
    "    'ecg_model_size': 'large',  # Options: 'small', 'base', 'large'\n",
    "    'ecg_embed_dim': 256,\n",
    "    'ecg_freeze': False,  # Set True to freeze pretrained weights\n",
    "    'ecg_use_pretrained': True,\n",
    "    \n",
    "    # Model - Tabular Encoder\n",
    "    'tabular_dim': 32,\n",
    "    'tabular_depth': 2,\n",
    "    'tabular_heads': 4,\n",
    "    'tabular_output_dim': 128,\n",
    "    \n",
    "    # Model - Fusion\n",
    "    'fusion_dim': 256,\n",
    "    \n",
    "    # Training\n",
    "    'num_epochs': 50,  # Reduced for Colab time limits\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'warmup_epochs': 3,\n",
    "    'dropout': 0.1,\n",
    "    \n",
    "    # Loss\n",
    "    'loss_type': 'asymmetric',  # Options: 'bce', 'focal', 'asymmetric'\n",
    "    'use_pos_weights': True,\n",
    "    \n",
    "    # Regularization\n",
    "    'early_stopping_patience': 10,\n",
    "    \n",
    "    # Computational\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'mixed_precision': True,  # Use AMP for faster training\n",
    "    \n",
    "    # Output\n",
    "    'output_dir': './outputs',\n",
    "    'save_freq': 5,\n",
    "    \n",
    "    # Evaluation\n",
    "    'eval_uncertainty': True,\n",
    "    'uncertainty_samples': 20,\n",
    "}\n",
    "\n",
    "print(\"Configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "training"
   },
   "source": [
    "## 4. Training\n",
    "\n",
    "Now let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "from src.models import SHDMultimodalModel\n",
    "from src.dataset import get_dataloaders, EchoNextDataset\n",
    "from src.utils import (\n",
    "    compute_metrics, compute_calibration_metrics,\n",
    "    FocalLoss, AsymmetricLoss, EarlyStopping,\n",
    "    save_checkpoint, load_checkpoint, AverageMeter,\n",
    "    get_pos_weights\n",
    ")\n",
    "\n",
    "print(\"‚úì Imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_data"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading datasets...\")\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    data_dir=config['data_dir'],\n",
    "    batch_size=config['batch_size'],\n",
    "    num_workers=config['num_workers']\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úì Data loaded:\")\n",
    "print(f\"  Training batches: {len(train_loader)}\")\n",
    "print(f\"  Validation batches: {len(val_loader)}\")\n",
    "print(f\"  Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "create_model"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "print(\"Creating model...\")\n",
    "\n",
    "device = torch.device(config['device'])\n",
    "\n",
    "model_config = {\n",
    "    'ecg_config': {\n",
    "        'model_size': config['ecg_model_size'],\n",
    "        'embed_dim': config['ecg_embed_dim'],\n",
    "        'freeze_encoder': config['ecg_freeze'],\n",
    "        'use_pretrained': config['ecg_use_pretrained'],\n",
    "        'pooling': 'mean',\n",
    "    },\n",
    "    'tabular_config': {\n",
    "        'dim': config['tabular_dim'],\n",
    "        'depth': config['tabular_depth'],\n",
    "        'heads': config['tabular_heads'],\n",
    "        'output_dim': config['tabular_output_dim'],\n",
    "        'attn_dropout': config['dropout'],\n",
    "        'ff_dropout': config['dropout'],\n",
    "    },\n",
    "    'fusion_config': {\n",
    "        'ecg_dim': config['ecg_embed_dim'],\n",
    "        'tabular_dim': config['tabular_output_dim'],\n",
    "        'output_dim': config['fusion_dim'],\n",
    "    },\n",
    "    'prediction_config': {\n",
    "        'input_dim': config['fusion_dim'],\n",
    "        'num_labels': len(EchoNextDataset.LABEL_COLUMNS),\n",
    "        'dropout': config['dropout'],\n",
    "    }\n",
    "}\n",
    "\n",
    "model = SHDMultimodalModel(**model_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Count parameters\n",
    "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n‚úì Model created with {num_params:,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_training"
   },
   "outputs": [],
   "source": [
    "# Setup training components\n",
    "print(\"Setting up training...\")\n",
    "\n",
    "# Loss function\n",
    "if config['use_pos_weights']:\n",
    "    train_dataset = train_loader.dataset\n",
    "    train_labels = train_dataset.labels\n",
    "    pos_weights = get_pos_weights(train_labels).to(device)\n",
    "    print(f\"Using positive weights for class imbalance\")\n",
    "\n",
    "if config['loss_type'] == 'bce':\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights if config['use_pos_weights'] else None)\n",
    "elif config['loss_type'] == 'focal':\n",
    "    criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "elif config['loss_type'] == 'asymmetric':\n",
    "    criterion = AsymmetricLoss(gamma_neg=4.0, gamma_pos=1.0)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=config['lr'],\n",
    "    weight_decay=config['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler with warmup\n",
    "num_training_steps = len(train_loader) * config['num_epochs']\n",
    "num_warmup_steps = len(train_loader) * config['warmup_epochs']\n",
    "\n",
    "def lr_lambda(current_step):\n",
    "    if current_step < num_warmup_steps:\n",
    "        return float(current_step) / float(max(1, num_warmup_steps))\n",
    "    return max(0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - num_warmup_steps)))\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = torch.cuda.amp.GradScaler() if config['mixed_precision'] else None\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=config['early_stopping_patience'],\n",
    "    mode='max'  # Maximize validation AUROC\n",
    ")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path(config['output_dir'])\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save configuration\n",
    "with open(output_dir / 'config.json', 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"‚úì Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_function"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, scaler=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    for batch in pbar:\n",
    "        waveform = batch['waveform'].to(device)\n",
    "        tabular = batch['tabular'].to(device)\n",
    "        tabular_mask = batch['tabular_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if scaler is not None:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                output = model(waveform, tabular, tabular_mask)\n",
    "                loss = criterion(output['logits'], labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            output = model(waveform, tabular, tabular_mask)\n",
    "            loss = criterion(output['logits'], labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        \n",
    "        losses.update(loss.item(), waveform.size(0))\n",
    "        pbar.set_postfix({'loss': losses.avg})\n",
    "    \n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    \"\"\"Evaluate model.\"\"\"\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_calibrated_probs = []\n",
    "    all_fusion_gates = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Evaluating', leave=False)\n",
    "    for batch in pbar:\n",
    "        waveform = batch['waveform'].to(device)\n",
    "        tabular = batch['tabular'].to(device)\n",
    "        tabular_mask = batch['tabular_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        output = model(waveform, tabular, tabular_mask)\n",
    "        loss = criterion(output['logits'], labels)\n",
    "        \n",
    "        losses.update(loss.item(), waveform.size(0))\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        all_probs.append(output['probs'].cpu().numpy())\n",
    "        all_calibrated_probs.append(output['calibrated_probs'].cpu().numpy())\n",
    "        all_fusion_gates.append(output['fusion_gates'].cpu().numpy())\n",
    "    \n",
    "    all_labels = np.concatenate(all_labels, axis=0)\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_calibrated_probs = np.concatenate(all_calibrated_probs, axis=0)\n",
    "    all_fusion_gates = np.concatenate(all_fusion_gates, axis=0)\n",
    "    \n",
    "    metrics = compute_metrics(all_labels, all_calibrated_probs, EchoNextDataset.LABEL_COLUMNS)\n",
    "    calibration_metrics = compute_calibration_metrics(all_labels, all_calibrated_probs)\n",
    "    \n",
    "    metrics.update(calibration_metrics)\n",
    "    metrics['loss'] = losses.avg\n",
    "    metrics['avg_ecg_gate'] = all_fusion_gates[:, 0].mean()\n",
    "    metrics['avg_tabular_gate'] = all_fusion_gates[:, 1].mean()\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "print(\"‚úì Training functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_loop"
   },
   "outputs": [],
   "source": [
    "# Main training loop\n",
    "print(\"Starting training...\\n\")\n",
    "\n",
    "# Load TensorBoard extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "# Start TensorBoard\n",
    "writer = SummaryWriter(log_dir=output_dir / 'logs')\n",
    "\n",
    "best_val_auroc = 0.0\n",
    "training_history = []\n",
    "\n",
    "for epoch in range(config['num_epochs']):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_epoch(\n",
    "        model, train_loader, criterion, optimizer, scheduler, device, scaler\n",
    "    )\n",
    "    \n",
    "    # Evaluate\n",
    "    val_metrics = evaluate(model, val_loader, criterion, device)\n",
    "    \n",
    "    # Log metrics\n",
    "    writer.add_scalar('Loss/train', train_loss, epoch)\n",
    "    writer.add_scalar('Loss/val', val_metrics['loss'], epoch)\n",
    "    writer.add_scalar('AUROC/val_macro', val_metrics.get('macro_auroc', 0), epoch)\n",
    "    writer.add_scalar('AUPRC/val_macro', val_metrics.get('macro_auprc', 0), epoch)\n",
    "    writer.add_scalar('Calibration/val_ece', val_metrics.get('mean_ece', 0), epoch)\n",
    "    writer.add_scalar('FusionGates/ecg', val_metrics['avg_ecg_gate'], epoch)\n",
    "    writer.add_scalar('FusionGates/tabular', val_metrics['avg_tabular_gate'], epoch)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_metrics['loss']:.4f}\")\n",
    "    print(f\"Val Macro AUROC: {val_metrics.get('macro_auroc', 0):.4f}\")\n",
    "    print(f\"Val Macro AUPRC: {val_metrics.get('macro_auprc', 0):.4f}\")\n",
    "    print(f\"Val ECE: {val_metrics.get('mean_ece', 0):.4f}\")\n",
    "    print(f\"Fusion Gates - ECG: {val_metrics['avg_ecg_gate']:.3f}, Tabular: {val_metrics['avg_tabular_gate']:.3f}\")\n",
    "    \n",
    "    # Save history\n",
    "    training_history.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_metrics['loss'],\n",
    "        'val_auroc': val_metrics.get('macro_auroc', 0),\n",
    "        'val_auprc': val_metrics.get('macro_auprc', 0),\n",
    "    })\n",
    "    \n",
    "    # Save best model\n",
    "    val_auroc = val_metrics.get('macro_auroc', 0)\n",
    "    if val_auroc > best_val_auroc:\n",
    "        best_val_auroc = val_auroc\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, val_metrics,\n",
    "            output_dir / 'best_model.pt'\n",
    "        )\n",
    "        print(f\"‚úì New best model saved (AUROC: {val_auroc:.4f})\")\n",
    "    \n",
    "    # Save checkpoint periodically\n",
    "    if (epoch + 1) % config['save_freq'] == 0:\n",
    "        save_checkpoint(\n",
    "            model, optimizer, epoch, val_metrics,\n",
    "            output_dir / f'checkpoint_epoch_{epoch+1}.pt'\n",
    "        )\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping(val_auroc):\n",
    "        print(f\"\\n‚ö†Ô∏è Early stopping triggered at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úì Training complete!\")\n",
    "print(f\"Best validation AUROC: {best_val_auroc:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tensorboard"
   },
   "source": [
    "### View TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show_tensorboard"
   },
   "outputs": [],
   "source": [
    "# Launch TensorBoard\n",
    "%tensorboard --logdir outputs/logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evaluation"
   },
   "source": [
    "## 5. Evaluation & Visualization\n",
    "\n",
    "Evaluate the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_eval"
   },
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "load_checkpoint(model, None, output_dir / 'best_model.pt', device)\n",
    "test_metrics = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "# Print test results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "print(f\"Test Macro AUROC: {test_metrics.get('macro_auroc', 0):.4f}\")\n",
    "print(f\"Test Macro AUPRC: {test_metrics.get('macro_auprc', 0):.4f}\")\n",
    "print(f\"Test ECE: {test_metrics.get('mean_ece', 0):.4f}\")\n",
    "print(f\"Test MCE: {test_metrics.get('mean_mce', 0):.4f}\")\n",
    "\n",
    "print(\"\\nPer-label AUROC:\")\n",
    "for label in EchoNextDataset.LABEL_COLUMNS:\n",
    "    auroc = test_metrics.get(f'{label}_auroc', None)\n",
    "    if auroc is not None:\n",
    "        print(f\"  {label}: {auroc:.4f}\")\n",
    "\n",
    "# Save test results\n",
    "with open(output_dir / 'test_results.json', 'w') as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úì Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_history"
   },
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = [h['epoch'] for h in training_history]\n",
    "train_losses = [h['train_loss'] for h in training_history]\n",
    "val_losses = [h['val_loss'] for h in training_history]\n",
    "val_aurocs = [h['val_auroc'] for h in training_history]\n",
    "\n",
    "# Loss plot\n",
    "axes[0].plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "axes[0].plot(epochs, val_losses, label='Val Loss', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUROC plot\n",
    "axes[1].plot(epochs, val_aurocs, label='Val AUROC', marker='o', color='green')\n",
    "axes[1].axhline(y=best_val_auroc, color='r', linestyle='--', label=f'Best: {best_val_auroc:.4f}')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('AUROC')\n",
    "axes[1].set_title('Validation AUROC')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'training_history.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Training history plotted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "plot_per_label"
   },
   "outputs": [],
   "source": [
    "# Plot per-label performance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = EchoNextDataset.LABEL_COLUMNS\n",
    "aurocs = [test_metrics.get(f'{label}_auroc', 0) for label in labels]\n",
    "auprcs = [test_metrics.get(f'{label}_auprc', 0) for label in labels]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, aurocs, width, label='AUROC', alpha=0.8)\n",
    "ax.bar(x + width/2, auprcs, width, label='AUPRC', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Label')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Per-Label Performance on Test Set')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([l.replace('_flag', '').replace('_', ' ')[:20] for l in labels], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'per_label_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úì Per-label performance plotted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download"
   },
   "source": [
    "## 6. Download Results\n",
    "\n",
    "Download trained model and results to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zip_results"
   },
   "outputs": [],
   "source": [
    "# Create a zip file with all results\n",
    "import shutil\n",
    "\n",
    "print(\"Creating results archive...\")\n",
    "\n",
    "# Create archive\n",
    "shutil.make_archive('shd_training_results', 'zip', output_dir)\n",
    "\n",
    "print(\"‚úì Archive created: shd_training_results.zip\")\n",
    "print(\"\\nContents:\")\n",
    "!unzip -l shd_training_results.zip | head -20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Download the results\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "files.download('shd_training_results.zip')\n",
    "\n",
    "print(\"\\n‚úì Download started!\")\n",
    "print(\"\\nThe archive contains:\")\n",
    "print(\"  - best_model.pt (trained model weights)\")\n",
    "print(\"  - config.json (training configuration)\")\n",
    "print(\"  - test_results.json (test metrics)\")\n",
    "print(\"  - training_history.png (loss/AUROC plots)\")\n",
    "print(\"  - per_label_performance.png (per-label metrics)\")\n",
    "print(\"  - logs/ (TensorBoard logs)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inference"
   },
   "source": [
    "## 7. Inference Example (Optional)\n",
    "\n",
    "Run inference on a single sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "single_inference"
   },
   "outputs": [],
   "source": [
    "# Get a single sample from test set\n",
    "model.eval()\n",
    "\n",
    "sample = test_loader.dataset[0]\n",
    "waveform = sample['waveform'].unsqueeze(0).to(device)\n",
    "tabular = sample['tabular'].unsqueeze(0).to(device)\n",
    "tabular_mask = sample['tabular_mask'].unsqueeze(0).to(device)\n",
    "true_labels = sample['labels'].numpy()\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    output = model(waveform, tabular, tabular_mask, return_embeddings=True)\n",
    "\n",
    "# Get predictions\n",
    "probs = output['calibrated_probs'].cpu().numpy()[0]\n",
    "fusion_gates = output['fusion_gates'].cpu().numpy()[0]\n",
    "\n",
    "print(\"Inference Results:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Fusion Gates - ECG: {fusion_gates[0]:.3f}, Tabular: {fusion_gates[1]:.3f}\")\n",
    "print(\"\\nPredictions:\")\n",
    "print(f\"{'Label':<50} {'True':<6} {'Pred':<6}\")\n",
    "print(\"-\"*60)\n",
    "for i, label in enumerate(EchoNextDataset.LABEL_COLUMNS):\n",
    "    print(f\"{label:<50} {int(true_labels[i]):<6} {probs[i]:.3f}\")\n",
    "\n",
    "print(\"\\n‚úì Inference complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "### Next Steps:\n",
    "1. **Review TensorBoard** for detailed training metrics\n",
    "2. **Download results** using the cells above\n",
    "3. **Experiment** with different hyperparameters\n",
    "4. **Fine-tune** on your specific use case\n",
    "\n",
    "### Tips for Better Performance:\n",
    "- Increase `num_epochs` for longer training (watch for overfitting)\n",
    "- Try different `ecg_model_size` ('small', 'base', 'large')\n",
    "- Adjust `batch_size` based on GPU memory\n",
    "- Experiment with different loss functions\n",
    "- Use `ecg_freeze=True` for faster training with frozen pretrained weights\n",
    "\n",
    "---\n",
    "\n",
    "**Questions or issues?** Check the project README or documentation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
